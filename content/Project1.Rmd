---
title: 'Project 1: Exploratory Data Analysis'
author: "SDS348 Fall 2019"
date: "9/16/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy=TRUE)
```
**Rob Bailey, rb38999**

## Data Wrangling and Data Exploration

### Instructions
A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 10/20/2019. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later!

I envision your written text forming something of a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be graded. Furthermore, all code contained in your final project document should work properly. Please do not include any extraneous code or code which produces error messages. (Code which produces warnings is acceptable, as long as you understand what the warnings mean).

### Find data:

Find two (!) datasets with one variable in common (e.g., dates, years, states, counties, countries), both with at least 50 observations (i.e., rows) in each. When combined, the resulting/final dataset must have **at least 4 different variables (at least 2 numeric) in addition to the common variable**.

Choose as many as you would like! If you found two datasets that you like but they don't have enough variables,  find a third dataset with the same common variable and join all three.


### Guidelines

1. If the datasets are not tidy, you will need to reshape them so that every observation has its own row and every variable its own column. If the datasets are both already tidy, you will make them untidy with pivot_wider()/spread() and then tidy them again with pivot_longer/gather() to demonstrate your use of the functions. It's fine to wait until you have your descriptives to use these functions (e.g., you might want to pivot_wider() to rearrange the data to make your descriptive statistics easier to look at); it's fine long as you use them at least once!

    - Depending on your datasets, it might be a good idea to do this before joining. For example, if you have a dataset you like with multiple measurements per year, but you want to join by year, you could average over your numeric variables to get means/year, do counts for your categoricals to get a counts/year, etc.
    

2. Join your 2+ separate data sources into a single dataset

    - You will document the type of join that you do (left/right/inner/full), including how many cases in each dataset were dropped and why you chose this particular join


3. Create summary statistics

    - Use *all six* core `dplyr` functions (filter, select, arrange, group_by, mutate, summarize) to manipulate and explore your dataset. For mutate, create a  new variable that is a function of at least one other variable, preferably using a dplyr vector function (see dplyr cheatsheet). It's fine to use the `_if`, `_at`, `_all` versions of mutate/summarize instead (indeed, it is encouraged if you have lots of variables)
    
    - Create summary statistics (mean, sd, var, n, quantile, min, max, n_distinct, cor, etc) for each of your numeric variables overall and after grouping by one of your categorical variables (either together or one-at-a-time; if you have two categorical variables, try to include at least one statistic based on a grouping of two categorical variables simultaneously). If you do not have any categorical variables, create one using mutate to satisfy the requirements above. Ideally, you will find a way to show these summary statistics in an easy-to-read table (e.g., by reshaping). If you have lots of numeric variables, or your categorical variables have too many categories, just pick a few (either numeric variables or categories of a categorical variable) and summarize based on those. It would be a good idea to show a correlation matrix for your numeric variables!
 
4. Make visualizations

    - If you have 5 variables (the minimum), with 2 numeric variables (the minimum), create at least two effective plots with ggplot that illustrate some of the more interesting findings that your descriptive statistics have turned up.
    - Each plot should have at least three variables mapped to separate aesthetics (if correlation heatmap, etc, fine to do the same "variable" on both axes)
    - At least one plot should include `stat="summary"`
    - Each plot should include a supporting paragraph describing the relationships that are being visualized and any notable trends that are apparent
        - It is fine to include more, but limit yourself to 4. Plots should avoid being redundant! Four bad plots will get a lower grade than two good plots, all else being equal.
    - If doing a 3D plot (not encouraged, but sometimes useful), it's fine to use plotly for one plot (make the other(s) in ggplot).
    
5. Perform k-means/PAM clustering or PCA on (at least) your numeric variables.

    - Include all steps as we discuss in class, including a visualization.

    - If you don't have at least 3 numeric variables, or you want to cluster based on categorical variables too, convert them to factors in R, generate Gower's dissimilarity matrix on the data, and do PAM clustering on the dissimilarities.
    
    - Show how you chose the final number of clusters/principal components 
    
    - Interpret the final clusters/principal components 

    
- For every step, you should document what your code does (in words) and what you see in the data.     
    
    
### Rubric

Prerequisite: Finding appropriate data from at least two sources per the instructions above: Failure to do this will result in a 0! You will submit a .Rmd file and a knitted document (pdf).

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidyr)
library(data.table)
library(psych)
```
### 0. Introduction (4  pts)

- Write a narrative introductory paragraph (or two) describing the datasets you have chosen, the variables they contain, how they were acquired, and why they are interesting to you. Expand on potential associations you may expect, if any.

*- The datasets I have chosen are about crimes per state in the United States, and the income and rent prices of each state as well. The reason I chose these datasets is because I am curious if there are striking correlations and relationships between these variables that can tell us more about crime rates in the United States, and why they might be higher in some states versus others.*

  *One of these datasets came from the `tidyr` package and is the us_rent_income dataset. This dataset contains variables on each state's estimated income and rent cost averages; in addition to this, it has included the 90% margin of error (moe) for verification of the data. The second dataset I'm using comes from statcrunch.com (owned and operated by Pearson) and is a dataset from 2005-2010 of crime rates (murder, rape, robbery, assault, burglary, etc.) per state in the US. In addition to this, there is information on college graduation rates, crime related expenditures, police expenditures, judicial expenditures, corrections expenditures, etc. Both of these datasets have some good variables to look into, and being able to look at correlation between income rates and crime rates in US states is an important matter.*
  
  *Though these datasets are not from the same years, they should offer a glimpse into the relationships that rent and income have with crime rates in various states. The us_rent_income dataset is from 2017, and the state_crime dataset is from years ranging from 2005-2010.*

### 1. Tidying: Rearranging (8 pts)

- Tidy the datasets (using the `tidyr` functions `pivot_longer`/`gather` and/or `pivot_wider`/`spread`) 
- If you data sets are already tidy, untidy them, retidy them.

```{r}

state_crime<- read.csv("~/Desktop/Rstudio/sds348_website/content/state_crime.csv")
state_incomerent<-environment(tidyr::us_rent_income)

#Split up the rent and income variables into columns with values, then take out 'moe' and 'GEOID'
incomerent<-us_rent_income%>%
  pivot_wider(names_from="variable",values_from="estimate")%>%
  select(-1,-3)%>%
  rename(state = NAME)

#Because both income and rent were set up to have specific values of 'moe' associated with them, when the variables were split apart by pivot_wider, it duplicated the state names, but not the data. So, half the state names had NAs for data. This is to remove the NAs:
incomerentna1 <- data.table(incomerent)[, lapply(.SD, function(x) x[order(is.na(x))])]

#This takes away the duplicate names of states and places it as its own Value
statena<-incomerentna1%>%
  mutate(state)%>%
  distinct(state)

statena<-statena[-c(52),] #remove Puerto Rico, which is not a state and incomplete income data (NA)

#Change the classification of `statena` from a Value to a dataframe in the Global Environment
statena<-as.data.frame(statena)

#take Puerto Rico out of the income and rent dataset, too
incomerentna1<-incomerentna1%>%
  na.omit()%>%
  select(-c(52),)

#bind the list of state names to the income and rent dataset that has duplicated state names
incomerentna <- cbind(incomerentna1, state1 = statena$state)

#take out the duplicated state name column, reorganize the columns, then replace it with the unduplicated one (`statena`)
incomerentna<-incomerentna[,c(4,1,2,3)]
incomerentna<-incomerentna%>% 
  select(-state)  

#rename the binded state variable to be 'state'
incomerentna<-incomerentna%>%
  rename(state = state1)

glimpse(incomerentna)
```

- Document the process (describe in words what was done)

*First, what I did was separate the rent and income variables per state into separate columns with the `incomerent` data by pivot_wider. When I separated this, there were NA's between the income values and rent values, because there were the associated margin of errors for each one. To avoid double readings of states (because of the attached moe to each variable of rent and income), I took out the margin of error variable. The potential issue of taking away the margin of error readings is that there will now be no way to account for the accuracy of the medians for income and rent of each state. After doing this, I took out the "GEOID" variable, because I'm only using state names to classify the states.*

    
### 2. Joining/Merging (8 pts)
- Join your datasets into one using a `dplyr` join function
- If you have multiple observations on the joining variable in either dataset, fix this by collapsing via summarize

```{r}
crimeincomejoin<-full_join(state_crime,incomerentna,by="state")%>%
  select(-medianinc)%>%
  filter(state != 'District of Columbia')

glimpse(crimeincomejoin)
```
- Discuss the process in words, including why you chose the join you did
- Discuss which cases were dropped, if any, and potential problems with this

*After the datasets were tidied up a bit, I conducted a full_join between `incomerentna` and `state_crime` by 'state'. The issue that I came across was the differences in income variables between the two datasets. The "medianinc" variable in `state_crime` is the median income for a family of 4 in 2005, and these medians are substantially higher than the median income per capita, which is where the "income" variable comes from in the `incomerentna` dataset. For this reason, I'm going to be using the "income" variable from `incomerentna`, because I believe income per capita is a more representative value of income crime rates per state. The potential issue associated with dropping the "medianinc" value is that there will be no way to know the median income per household in these states, but in each state, there are generally more individual/autonomous adults than there are number of family units. Family units tend to make higher median incomes, because they have children to provide for, whereas individual adults have to only provide for themselves. In addition to this, I removed the 'District of Columbia' data, because D.C. is not a state. Although it would be interesting to view crime and income in these areas, I feel I would have to also include all other U.S. territories to be fair.*

### 3. Wrangling (40 pts)

- Use all six core `dplyr` functions (filter, select, arrange, group_by, mutate, summarize) in the service of generating summary statistics (18 pts)

**Population sizes of each state in descending order**
```{r}
crimeincomejoin%>%
  select(state, pop, income)%>%
  arrange(desc(pop))%>%
  glimpse()
```

**High school graduation rates per state in descending order**
```{r}
crimeincomejoin%>%
  select(state, hsgrad)%>%
  arrange(desc(hsgrad))%>%
  glimpse()
```

**Unemployment rates per state in descending order**
```{r}
crimeincomejoin%>%
  select(state, pop, income, unemp)%>%
  arrange(desc(unemp))%>%
  glimpse()
```

**Total justice expenditures per state in descending order**
```{r}
crimeincomejoin%>%
  select(state, pop, totjustice, income)%>%
  arrange(desc(totjustice))%>%
  glimpse()
```

```{r}
expendprobs<-crimeincomejoin%>%
  mutate(totexpend= totjustice-police-judicial-corrections)%>%
  group_by(state,pop,totjustice,police,judicial,corrections,totexpend)%>%
  arrange(desc(totexpend!=0))%>%
  summarize(expendmiscalc=totexpend!=0)%>%
  arrange(desc(expendmiscalc==TRUE))
```
*The dataset I created here shows the miscalculations in total expenditures of each state. These errors in total state justice expenditures most likely came from rounding errors since they are all either + or - 1.*


- Use mutate to generate a variable that is a function of at least one other variable
```{r}
hsgradsnocoll<- crimeincomejoin%>%
  select(state,pop,hsgrad,collgrad)%>%
  mutate(hsgrad-collgrad)%>%
  arrange(desc(hsgrad-collgrad))

glimpse(hsgradsnocoll)

```

*Because everyone who goes to college has to have graduated highschool prior to acceptance, I was able to create a variable that shows how many highschool graduates do not end up graduating college. This rate statistic can be useful to understand the efficacy of the education path of each state, whether or not the people never attended college in the first place, or just didn't finish a degree.*


- Compute at least 10 different summary statistics using summarize and summarize with group_by (18 pts)
    - At least 2 of these should group by a categorical variable. Create one by dichotomizing a numeric if necessary
    - If applicable, at least 1 of these 5 should group by two categorical variables
    - Strongly encouraged to create a correlation matrix with `cor()` on your numeric variables
    
```{r}
expendprobs<-crimeincomejoin%>%
  mutate(totexpend= totjustice-police-judicial-corrections)%>%
  group_by(state,pop,totjustice,police,judicial,corrections,totexpend)%>%
  arrange(desc(totexpend!=0))%>%
  summarize(expendmiscalc=totexpend!=0)%>%
  arrange(desc(expendmiscalc==TRUE))

glimpse(expendprobs)
```

*The dataset I created here shows the miscalculations in total expenditures of each state. These errors in total state justice expenditures might have come from rounding errors since they are all either + or - 1. Without the ability to know, however, I will treat them as miscalculations of expenditures. Only 17 out of the 50 states had these issues, so it was still a minority who had expenditure miscalculations.*

```{r}
crimestats <- crimeincomejoin %>%
    select(murder,rape,robbery,assault,propertycr,burglary,larceny,vehtheft) %>%
    psych::describe(quant=c(.25,.75)) %>%
    as_tibble(rownames="crime")%>%
    select(-n,-kurtosis)%>%
    glimpse()
```

*I sorted the data and split it up to where the main statistics of each crime are shown in a tibble. This is important for viewing things like the highest and lowest rates of certain crimes, among other things. *

```{r}
crimeincomejoin%>%
  select(c(murder,rape,robbery,assault))%>%
  cor()
```

*This shows the correlation of the more violent crimes with each other.*

```{r}
crimeincomejoin%>%
  select(c(larceny,vehtheft,propertycr,burglary))%>%
  cor()
```

*This shows the correlation of theft crime rates with each other. This matrix makes sense, because property crime rates and larceny are very highly correlated, as are burglary and property crimes.*

```{r}
crimeincomejoin%>%
  select(c(larceny,vehtheft,propertycr,burglary,rent,income))%>%
  cor()
```
*This shows the correlation of crimes of theft category and rent and income. This insight is very interesting, because it shows that income and burglary are fairly strongly negatively correlated with each other.*


```{r}
crimeincomejoin%>%
  select(c(pop,police,judicial,corrections))%>%
  cor()
```

*These are correlations of the expenditures of the justice process with each other. As one would expect, these expenditures correlate pretty high with each other, because the more you spend on the judicial system, the more you spend on corrections. The more that is spent on police, the more is spent on the justice process, because chances are, more crimes are being caught with a larger police presence. *

```{r}
crimeincomejoin%>%
  group_by(state)%>%
  summarize(max(rent))%>%
  arrange(desc(`max(rent)`))%>%
  glimpse()
```

*This summary statistic shows the highest averages of rent per state, in descending order.*

```{r}
crimeincomejoin%>%
  group_by(state)%>%
  summarize(max(income))%>%
  arrange(desc(`max(income)`))%>%
  glimpse()
```

*This summary statistic shows the highest median incomes of individual adults per state, in descending order.*

```{r}
crimeincomejoin%>%
  group_by(state)%>%
  mutate(incomeleft=income-netelectric-rent)%>%
  summarize(incomeleft)%>%
  glimpse()
```

*This value "incomeleft" is the income that the average person might have after taking into account the costs of rent and of net electric. I wanted to include prices of gas, but with the travel per person in each state varying so greatly, there isn't a reliable statistic for the average distance traveled in gasoline-using machines, not to mention large differences in mpg's.*

```{r}



```


### 4. Visualizing (30 pts)

- Create two effective, polished plots with ggplot
    
```{r}
ggplot(crimeincomejoin, aes(state,burglary)) + geom_bar(aes(y = burglary,fill=income), stat = "summary") +scale_fill_gradient(low="dark blue",high="light pink")+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous() + ggtitle("Burglary Rates per state with Income data")
```
```{r}
ggplot(crimeincomejoin, aes(state,burglary)) + geom_bar(aes(y = burglary,fill=rent), stat = "summary") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous() + ggtitle("Burglary Rates per state with Rent data")
```

*The first graph in this sequence represents the burglary rates per state with overlayed median income data of each state. From this graph, it seems clear that the states with high burglary rates have lower median incomes. On the contrary, the states with the highest median incomes have some of the lowest burglary rates. This data visualization was imporant, because this is the relationship I was expecting to find from merging these two datasets in the first place.*

*Below this graph, I included another bar graph comparing burglary rates per state with rent data. Rent prices tend to be lower in more impoverished areas, but this varies greatly with gentrification of areas and states with huge socio-economic gaps. From this graph, however, one can see that the states with higher burglary rates have lower average rent prices.*

*Of course there are other crime statistics that I did not include in this visual representation, but burglary is one of the crimes in `crimeincomejoin` that is nonviolent and is a representation of theft rates. *

```{r}
ggplot(crimeincomejoin, aes(state,collgrad)) + geom_bar(aes(y = collgrad,fill=income), stat = "identity") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous() +ggtitle("College Graduation rates per state with Median Income data")
```

*The graph above shows the graduation rates per state with income data overlayed into each bar on the graph. The relationship that becomes apparent once this is done is the positive correlation between high college graduation rates and high median income values. Because college graduation rate is already a rate per state, I chose to use "stat="identity"". Connecticut, Maryland, Massachusetts, and New Jersey have the top college graduation rates out of the 50 states, and by coloration, they all have the highest median incomes. What's interesting to me about this relationship is that these states aren't states that have exceptionally high population rates. College acceptance rates for bigger states are more competitive, and because college acceptance rates ultimately effect college graduation rates, the larger states have lower college graduation rates. With these datasets, I can't quite test some of these assumptions, but from this graph, the two biggest states by population, California and Texas, don't have the highest college graduation rates.*
    
    
### 5. Dimensionality Reduction (20 pts) 

- Either k-means/PAM clustering or PCA (inclusive "or") should be performed on at least three numeric variables in your dataset

```{r}
cij1<-crimeincomejoin%>%select(-c(state,stabbr,pop1000_2005,police,judicial,corrections,unleadedgas,netelectric,totjustice,pop,unemp))
cij_nums<-cij1%>%select_if(is.numeric)%>%scale
rownames(cij_nums)<-cij1$state
cij_pca<-princomp(cij_nums)
names(cij_pca)
```

```{r}
summary(cij_pca, loadings=T)


eigval<-cij_pca$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval),2) #proportion of var explained by each PC

#scree plot to visually show variance that is explained by each PC group
ggplot()+geom_bar(aes(y=varprop,x=1:12),stat="identity")+xlab("")+geom_path(aes(y=varprop,x=1:12))+
  geom_text(aes(x=1:12,y=varprop,label=round(varprop,2)),vjust=1,col="white",size=5)+
  scale_y_continuous(breaks=seq(0,.6,.2),labels = scales::percent)+
  scale_x_continuous(breaks=1:10)

```
```{r}
round(cumsum(eigval)/sum(eigval),2) #cumulative proportion of variance
eigval #eigenvalues

eigen(cor(cij_nums))
```

```{r}
cijdf<-data.frame(PC1=cij_pca$scores[,1], PC2=cij_pca$scores[,2])
ggplot(cijdf,aes(PC1, PC2))+geom_point()
```

```{r}
ggplot(cijdf,aes(PC1, PC2, PC3))+geom_point()+
  stat_ellipse(data=cijdf[cijdf$PC1>  5.0,], aes(PC1, PC2),color="blue")+
  stat_ellipse(data=cijdf[cijdf$PC1< -3.8,], aes(PC1, PC2),color="blue")+
  stat_ellipse(data=cijdf[cijdf$PC2> 2.75,], aes(PC1, PC2),color="red")+
  stat_ellipse(data=cijdf[cijdf$PC2< -3.5,], aes(PC1, PC2),color="red")
```

```{r}
cij_pca$loadings[1:12,1:2]%>%as.data.frame%>%rownames_to_column%>%
ggplot()+geom_hline(aes(yintercept=0),lty=2)+
  geom_vline(aes(xintercept=0),lty=2)+ylab("PC2")+xlab("PC1")+
  geom_segment(aes(x=0,y=0,xend=Comp.1,yend=Comp.2),arrow=arrow(),col="purple")+
  geom_label(aes(x=Comp.1*1.1,y=Comp.2*1.1,label=rowname))
```

*This process was largely influenced by the code we completed in WS11, and it was helpful in making these plots above. The datasets that I used for this project ended up being a lot less correlated with each other than I thought they would be. Based on the initial scree plot, the variance wasn't tremendously explained by each PCA (PC1 explained approximately 46% of the variance). Because of this, the following scatterplot that was made with the first two PCs didn't have enough data to creat ellipses over the PCs. This was also due to the small size of this dataset, but it was still disappointing to see. There were no defined clusters of points that could have been PCs, so the next graph to look at was a graph showing correlation between the variables of these PCs. This graph was pretty telling, because certain crimes are very correlated with each other, while other crimes are vrey far off from the others (e.g. rape). What was particularly interesting to me about this last graph was the high correlation of college graduation rates and median income values. The high correlation of rates of larceny, burglary, and property crimes also makes sense, because they can be intermingled when classifying crimes of theft. I still have a lot to learn about PCA and using eigenvalues, among other things, and I hope we get more chances to practice this in the future!*
    

...